{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy praw google-api-python-client transformers torch pandas numpy matplotlib folium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_EbXKSQbQr1",
        "outputId": "1eecc643-0b50-4b8a-bf5a-59b2b125a69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.164.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw transformers torch pandas numpy matplotlib folium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kntoenq8cjgU",
        "outputId": "82b4a898-22b1-49f2-c37b-d6d0fbf99b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_client_id = \"cFfHjWEEzqDVNiSkXicfwA\"\n",
        "reddit_client_secret = \"YpJR2-0eeSoqc3A3d1EWwhYdM-O2-w\"\n",
        "reddit_user_agent = \"CloudSEK_Hackathon by /u/Jazzlike-Age6536\""
      ],
      "metadata": {
        "id": "V-doHtJRetMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Reddit\n",
        "reddit = praw.Reddit(\n",
        "    client_id=reddit_client_id,\n",
        "    client_secret=reddit_client_secret,\n",
        "    user_agent=reddit_user_agent\n",
        ")\n",
        "\n",
        "# Verify authentication\n",
        "try:\n",
        "    print(\"Authenticated as:\", reddit.user.me())\n",
        "except Exception as e:\n",
        "    print(\"Authentication check failed:\", e)\n",
        "\n",
        "# Search subreddits with error handling\n",
        "subreddits = \"worldnews+geopolitics+india\"\n",
        "reddit_posts = []\n",
        "try:\n",
        "    for submission in reddit.subreddit(subreddits).search(\"India Pakistan terrorism OR Kashmir conflict OR border attack\", limit=50):\n",
        "        reddit_posts.append({\n",
        "            \"platform\": \"Reddit\",\n",
        "            \"text\": submission.title + \" \" + submission.selftext,\n",
        "            \"user\": submission.author.name if submission.author else \"Anonymous\",\n",
        "            \"location\": \"N/A\",\n",
        "            \"created_at\": submission.created_utc\n",
        "        })\n",
        "    print(f\"Successfully collected {len(reddit_posts)} posts.\")\n",
        "except Exception as e:\n",
        "    print(\"Error during scraping:\", e)\n",
        "    raise\n",
        "\n",
        "# Save to DataFrame\n",
        "reddit_df = pd.DataFrame(reddit_posts)\n",
        "reddit_df.to_csv(\"reddit_osint_data.csv\", index=False)\n",
        "print(\"Data saved to reddit_osint_data.csv\")\n",
        "print(reddit_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "NFLguvCJgBlI",
        "outputId": "64cc2b7b-5074-4f05-e5ba-4b409bfa6df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'praw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7f7d603cf2dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Authenticate with Reddit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m reddit = praw.Reddit(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timeout-decorator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ryCdWXQjf5m",
        "outputId": "11d224c2-a29f-4ec0-a1ad-e79222282dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timeout-decorator\n",
            "  Downloading timeout-decorator-0.5.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: timeout-decorator\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-py3-none-any.whl size=5006 sha256=64b7fba237c620828ce1d3c2f95cc837c65041984555075ea3e4778e5afd5197\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/cd/d1/51736c6b95846b2613a520ce146a8f305c4016a987bc9faec7\n",
            "Successfully built timeout-decorator\n",
            "Installing collected packages: timeout-decorator\n",
            "Successfully installed timeout-decorator-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"reddit_osint_data.csv\")\n",
        "data = data.head(50)\n",
        "\n",
        "# Step 4.1: Threat Detection using Toxic Detection + Keywords\n",
        "print(\"Running threat detection...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
        "    classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "    print(\"Using unitary/toxic-bert model\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load toxic model: {e}. Falling back to cardiffnlp...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "# Function to truncate text\n",
        "def truncate_text(text, max_len=max_length):\n",
        "    encoded = tokenizer.encode(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    decoded = tokenizer.decode(encoded[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# Define refined threat-related keywords\n",
        "threat_keywords = [\n",
        "    \"terrorist attack\", \"bomb explosion\", \"militant strike\", \"violence outbreak\",\n",
        "    \"kill operation\", \"assault plan\", \"terror plot\",\"terrorist\", \"armed conflict\"\n",
        "]\n",
        "\n",
        "# Check for threat keywords in text\n",
        "def has_threat_keywords(text):\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in threat_keywords)\n",
        "\n",
        "# Classify posts with toxic model\n",
        "def classify_text(text):\n",
        "    try:\n",
        "        truncated_text = truncate_text(text)\n",
        "        result = classifier(truncated_text)[0]\n",
        "        if \"toxic-bert\" in classifier.model.name_or_path:\n",
        "            # Map toxic classification to threat score\n",
        "            return result[\"score\"] if result[\"label\"] == \"toxic\" else 0\n",
        "        else:  # Fallback to cardiffnlp\n",
        "            result = classifier(truncated_text)[0]\n",
        "            if result[\"label\"] == \"negative\":\n",
        "                return result[\"score\"]\n",
        "            elif result[\"label\"] == \"neutral\":\n",
        "                return result[\"score\"] * 0.5\n",
        "            else:\n",
        "                return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying text: {text[:50]}... | Error: {e}\")\n",
        "        return 0\n",
        "\n",
        "data[\"threat_score\"] = data[\"text\"].apply(classify_text)\n",
        "# Flag threats based on BOTH score > 0.5 AND keywords\n",
        "data[\"is_threat\"] = (data[\"threat_score\"] > 0.5) & data[\"text\"].apply(has_threat_keywords)\n",
        "\n",
        "# Print all posts with threat scores\n",
        "print(\"All Posts with Threat Scores:\")\n",
        "print(data[[\"platform\", \"text\", \"threat_score\", \"is_threat\"]])\n",
        "\n",
        "# Step 4.2: Entity Extraction (Locations)\n",
        "print(\"\\nExtracting locations...\")\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "def truncate_text_for_ner(text, max_len=max_length):\n",
        "    encoded = ner_tokenizer.encode(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    decoded = ner_tokenizer.decode(encoded[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "def extract_entities(text):\n",
        "    try:\n",
        "        truncated_text = truncate_text_for_ner(text)\n",
        "        entities = ner(truncated_text)\n",
        "        locations = [e[\"word\"] for e in entities if e[\"entity_group\"] == \"LOC\"]\n",
        "        cleaned_locations = [re.sub(r'##', '', loc) for loc in locations]\n",
        "        return \", \".join(cleaned_locations) if cleaned_locations else \"N/A\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting entities from text: {text[:50]}... | Error: {e}\")\n",
        "        return \"N/A\"\n",
        "\n",
        "data[\"locations\"] = data[\"text\"].apply(extract_entities)\n",
        "print(\"Posts with Extracted Locations:\")\n",
        "print(data[[\"platform\", \"text\", \"locations\"]])\n",
        "\n",
        "# Save the analyzed data\n",
        "data.to_csv(\"reddit_osint_analyzed.csv\", index=False)\n",
        "print(\"Analyzed data saved to reddit_osint_analyzed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdTVyAOToRRl",
        "outputId": "77b879a9-de94-4d66-82b2-7093a30bb677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running threat detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using unitary/toxic-bert model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Posts with Threat Scores:\n",
            "   platform                                               text  threat_score  \\\n",
            "0    Reddit  [R] Megathread III: India-Pakistan Border Skir...      0.000766   \n",
            "1    Reddit  Are there growing signs of instability in Paki...      0.002829   \n",
            "2    Reddit  A rundown of the ideological roots and history...      0.000699   \n",
            "3    Reddit  International Diplomatic Response to Indian Ai...      0.000712   \n",
            "4    Reddit  Neoliberalism, Crony Capitalists, And The Indi...      0.000819   \n",
            "5    Reddit  What are some of the mind blowing operations o...      0.002264   \n",
            "6    Reddit  Found something that you should know about Roh...      0.001188   \n",
            "7    Reddit  India-Pakistan tensions rise as India announce...      0.000875   \n",
            "8    Reddit  Pahalgam terror attack: UN urges India and Pak...      0.002354   \n",
            "9    Reddit  India-Pakistan Tensions On Verge Of Erupting I...      0.028776   \n",
            "10   Reddit  Kashmiri students attacked in Punjab as India ...      0.005165   \n",
            "11   Reddit  In a secret deal, Pakistan allowed American dr...      0.000937   \n",
            "12   Reddit  A group of Indians were filmed chanting ‘death...      0.088193   \n",
            "13   Reddit  Key ISI Figure Behind 2018 J&K Attack, Retired...      0.000922   \n",
            "14   Reddit  Religious hate spreading among Indian Youth Hi...      0.005225   \n",
            "15   Reddit  India Summons Pakistan Diplomat Over Multiple ...      0.001578   \n",
            "16   Reddit  Pakistan shoots down two Indian aircraft insid...      0.029397   \n",
            "17   Reddit  Pakistan bans Bollywood films in the country a...      0.002111   \n",
            "18   Reddit  Megathread: India-Pakistan border skirmish The...      0.000913   \n",
            "19   Reddit  Pakistan based terror group attacks hindu pilg...      0.027688   \n",
            "20   Reddit  Indian Air Force Carries Out Strike On Terror ...      0.001883   \n",
            "21   Reddit  Pakistan hits out at US and India after Biden-...      0.001456   \n",
            "22   Reddit              Pakistan frees captured Indian pilot       0.019363   \n",
            "23   Reddit  Government proposes to remove Article 370, bif...      0.000837   \n",
            "24   Reddit  Pahalgam terror attack: Why now, Pakistan Army...      0.001276   \n",
            "25   Reddit  How India can punish Pakistan after Pahalgam a...      0.024462   \n",
            "26   Reddit  In a First, Pakistan warns India of terrorist ...      0.000702   \n",
            "27   Reddit  Pathankot attack was staged by India, says Pak...      0.000790   \n",
            "28   Reddit  Pakistan released a U.S.-wanted militant with ...      0.008361   \n",
            "29   Reddit  17 Indian soldiers killed in a terrorist attac...      0.005345   \n",
            "30   Reddit  Indian politician boasts about getting Muslims...      0.334291   \n",
            "31   Reddit  Despite countless dossiers by India, Pakistan ...      0.000704   \n",
            "32   Reddit  International Reactions to Orlando Tragedy Thi...      0.001073   \n",
            "33   Reddit  What would happen if North Korea attacks South...      0.111765   \n",
            "34   Reddit  Minority places of worship were attacked in In...      0.000865   \n",
            "35   Reddit  Pakistan-based terror groups planning to attac...      0.001357   \n",
            "36   Reddit  Pakistan bans Christian TV stations in 'attack...      0.006837   \n",
            "37   Reddit  A common person's view of BJP government since...      0.001004   \n",
            "38   Reddit  \"Deeply Dissatisfied\" With India Position: Ukr...      0.000853   \n",
            "39   Reddit  Pulwama terror attack: India hikes customs dut...      0.004259   \n",
            "40   Reddit  India strikes 37 Pakistani outposts, kills 15 ...      0.118596   \n",
            "41   Reddit  Bollywood actress Veena Malik sentenced to 26 ...      0.038441   \n",
            "42   Reddit  'Order forces to attack India': PoK Prime Mini...      0.004501   \n",
            "43   Reddit  Arundhati Roy who said \"kashmir never integral...      0.001053   \n",
            "44   Reddit  Indian Soldiers Dancing on Abandoned Pakistani...      0.002676   \n",
            "45   Reddit  India conducted surgical strikes last night ac...      0.002285   \n",
            "46   Reddit  Pakistan court orders release of Islamist accu...      0.002910   \n",
            "47   Reddit  On this day, Pakistan launched Operation Cheng...      0.001183   \n",
            "48   Reddit  Pakistan 'can and must' dismantle all terror n...      0.020276   \n",
            "49   Reddit  HafizSaeedJUD: #KarachiAirport is an attack on...      0.005704   \n",
            "\n",
            "    is_threat  \n",
            "0       False  \n",
            "1       False  \n",
            "2       False  \n",
            "3       False  \n",
            "4       False  \n",
            "5       False  \n",
            "6       False  \n",
            "7       False  \n",
            "8       False  \n",
            "9       False  \n",
            "10      False  \n",
            "11      False  \n",
            "12      False  \n",
            "13      False  \n",
            "14      False  \n",
            "15      False  \n",
            "16      False  \n",
            "17      False  \n",
            "18      False  \n",
            "19      False  \n",
            "20      False  \n",
            "21      False  \n",
            "22      False  \n",
            "23      False  \n",
            "24      False  \n",
            "25      False  \n",
            "26      False  \n",
            "27      False  \n",
            "28      False  \n",
            "29      False  \n",
            "30      False  \n",
            "31      False  \n",
            "32      False  \n",
            "33      False  \n",
            "34      False  \n",
            "35      False  \n",
            "36      False  \n",
            "37      False  \n",
            "38      False  \n",
            "39      False  \n",
            "40      False  \n",
            "41      False  \n",
            "42      False  \n",
            "43      False  \n",
            "44      False  \n",
            "45      False  \n",
            "46      False  \n",
            "47      False  \n",
            "48      False  \n",
            "49      False  \n",
            "\n",
            "Extracting locations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posts with Extracted Locations:\n",
            "   platform                                               text  \\\n",
            "0    Reddit  [R] Megathread III: India-Pakistan Border Skir...   \n",
            "1    Reddit  Are there growing signs of instability in Paki...   \n",
            "2    Reddit  A rundown of the ideological roots and history...   \n",
            "3    Reddit  International Diplomatic Response to Indian Ai...   \n",
            "4    Reddit  Neoliberalism, Crony Capitalists, And The Indi...   \n",
            "5    Reddit  What are some of the mind blowing operations o...   \n",
            "6    Reddit  Found something that you should know about Roh...   \n",
            "7    Reddit  India-Pakistan tensions rise as India announce...   \n",
            "8    Reddit  Pahalgam terror attack: UN urges India and Pak...   \n",
            "9    Reddit  India-Pakistan Tensions On Verge Of Erupting I...   \n",
            "10   Reddit  Kashmiri students attacked in Punjab as India ...   \n",
            "11   Reddit  In a secret deal, Pakistan allowed American dr...   \n",
            "12   Reddit  A group of Indians were filmed chanting ‘death...   \n",
            "13   Reddit  Key ISI Figure Behind 2018 J&K Attack, Retired...   \n",
            "14   Reddit  Religious hate spreading among Indian Youth Hi...   \n",
            "15   Reddit  India Summons Pakistan Diplomat Over Multiple ...   \n",
            "16   Reddit  Pakistan shoots down two Indian aircraft insid...   \n",
            "17   Reddit  Pakistan bans Bollywood films in the country a...   \n",
            "18   Reddit  Megathread: India-Pakistan border skirmish The...   \n",
            "19   Reddit  Pakistan based terror group attacks hindu pilg...   \n",
            "20   Reddit  Indian Air Force Carries Out Strike On Terror ...   \n",
            "21   Reddit  Pakistan hits out at US and India after Biden-...   \n",
            "22   Reddit              Pakistan frees captured Indian pilot    \n",
            "23   Reddit  Government proposes to remove Article 370, bif...   \n",
            "24   Reddit  Pahalgam terror attack: Why now, Pakistan Army...   \n",
            "25   Reddit  How India can punish Pakistan after Pahalgam a...   \n",
            "26   Reddit  In a First, Pakistan warns India of terrorist ...   \n",
            "27   Reddit  Pathankot attack was staged by India, says Pak...   \n",
            "28   Reddit  Pakistan released a U.S.-wanted militant with ...   \n",
            "29   Reddit  17 Indian soldiers killed in a terrorist attac...   \n",
            "30   Reddit  Indian politician boasts about getting Muslims...   \n",
            "31   Reddit  Despite countless dossiers by India, Pakistan ...   \n",
            "32   Reddit  International Reactions to Orlando Tragedy Thi...   \n",
            "33   Reddit  What would happen if North Korea attacks South...   \n",
            "34   Reddit  Minority places of worship were attacked in In...   \n",
            "35   Reddit  Pakistan-based terror groups planning to attac...   \n",
            "36   Reddit  Pakistan bans Christian TV stations in 'attack...   \n",
            "37   Reddit  A common person's view of BJP government since...   \n",
            "38   Reddit  \"Deeply Dissatisfied\" With India Position: Ukr...   \n",
            "39   Reddit  Pulwama terror attack: India hikes customs dut...   \n",
            "40   Reddit  India strikes 37 Pakistani outposts, kills 15 ...   \n",
            "41   Reddit  Bollywood actress Veena Malik sentenced to 26 ...   \n",
            "42   Reddit  'Order forces to attack India': PoK Prime Mini...   \n",
            "43   Reddit  Arundhati Roy who said \"kashmir never integral...   \n",
            "44   Reddit  Indian Soldiers Dancing on Abandoned Pakistani...   \n",
            "45   Reddit  India conducted surgical strikes last night ac...   \n",
            "46   Reddit  Pakistan court orders release of Islamist accu...   \n",
            "47   Reddit  On this day, Pakistan launched Operation Cheng...   \n",
            "48   Reddit  Pakistan 'can and must' dismantle all terror n...   \n",
            "49   Reddit  HafizSaeedJUD: #KarachiAirport is an attack on...   \n",
            "\n",
            "                                            locations  \n",
            "0   India, Pakistan, b, alakot, art, hin, art, Ind...  \n",
            "1   Pakistan, Pakistan, Afghanistan, Afghanistan, ...  \n",
            "2   Kashmir, Kashmir, India, Pakistan, Pakistan, I...  \n",
            "3   Pakistan, China, India, Pakistan, South Asia, ...  \n",
            "4   South Asia, South Asia, Pakistan, India, India...  \n",
            "5   India, Sikkim, Tibet, Nepal, Bhutan, West Beng...  \n",
            "6   India, Myanmar, Burma, Rakhine State, Myanmar,...  \n",
            "7   India, Pakistan, India, b, ais, aran Valley, P...  \n",
            "8                                     India, Pakistan  \n",
            "9                                     India, Pakistan  \n",
            "10                            Punjab, India, Pakistan  \n",
            "11                                    Pakistan, India  \n",
            "12           Pakistan, Leicester, UK, India, Pakistan  \n",
            "13                                           Pakistan  \n",
            "14                                           Pakistan  \n",
            "15                                    India, Pakistan  \n",
            "16                                           Pakistan  \n",
            "17                                    Pakistan, India  \n",
            "18  India, Pakistan, Pakistan, Kashmir, Pakistan, ...  \n",
            "19                             Pakistan, Reasi, India  \n",
            "20                                  Pakistan, Kashmir  \n",
            "21            Pakistan, US, India, Pakistan, Pakistan  \n",
            "22                                           Pakistan  \n",
            "23  Jammu & Kashmir, Jammu, Kashmir, Jammu, Kashmi...  \n",
            "24                                     Kashmir, India  \n",
            "25                                    India, Pakistan  \n",
            "26                                    Pakistan, India  \n",
            "27                         Pathankot, India, Pakistan  \n",
            "28                         Pakistan, ., Mumbai, India  \n",
            "29                                                N/A  \n",
            "30                                                N/A  \n",
            "31                                    India, Pakistan  \n",
            "32  Orlando, Orlando, US, US, Vatican, France, Flo...  \n",
            "33  North Korea, South Korea, China, Taiwan, as, P...  \n",
            "34                             India, Pakistan, India  \n",
            "35                                          India, US  \n",
            "36                                           Pakistan  \n",
            "37                                                N/A  \n",
            "38                             India, Ukraine, Russia  \n",
            "39                                    India, Pakistan  \n",
            "40                                              India  \n",
            "41                                                N/A  \n",
            "42                                    India, Pakistan  \n",
            "43  India, New Delhi, Kashmir, Kashmir, Kashmir, D...  \n",
            "44  Longewala, Rajasthan, Longewala, Pakistan, Jai...  \n",
            "45                                         India, LoC  \n",
            "46             Pakistan, United States, India, Mumbai  \n",
            "47                             Pakistan, India, India  \n",
            "48                                           Pakistan  \n",
            "49                                    Pakistan, India  \n",
            "Analyzed data saved to reddit_osint_analyzed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load data (limited to 20 posts)\n",
        "data = pd.read_csv(\"reddit_osint_data.csv\")\n",
        "data = data.head(20)\n",
        "\n",
        "# Step 4.1: Threat Detection using Toxic Detection + Keywords\n",
        "print(\"Running threat detection...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
        "    classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "    print(\"Using unitary/toxic-bert model\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load toxic model: {e}. Falling back to nlptown sentiment...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "# Function to truncate text\n",
        "def truncate_text(text, max_len=max_length):\n",
        "    encoded = tokenizer.encode(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    decoded = tokenizer.decode(encoded[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# Define threat-related keywords\n",
        "threat_keywords = [\n",
        "    \"terrorist\", \"attack\", \"bomb\", \"explosion\", \"militant\", \"strike\", \"violence\",\n",
        "    \"kill\", \"assault\", \"terror\", \"conflict\", \"death\", \"threat\", \"war\"\n",
        "]\n",
        "\n",
        "# Check for any threat keyword in text\n",
        "def has_threat_keywords(text):\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in threat_keywords)\n",
        "\n",
        "# Classify posts with toxic model\n",
        "def classify_text(text):\n",
        "    try:\n",
        "        truncated_text = truncate_text(text)\n",
        "        result = classifier(truncated_text)[0]\n",
        "        if \"toxic-bert\" in classifier.model.name_or_path:\n",
        "            score = result[\"score\"] if result[\"label\"] == \"toxic\" else 0\n",
        "        else:  # Fallback to nlptown sentiment\n",
        "            # This model outputs star ratings (1 to 5 stars); map to threat score\n",
        "            label = int(result[\"label\"].split()[0])  # e.g., \"1 star\" → 1\n",
        "            score = (5 - label) / 4  # Map 1 star to 1 (high threat), 5 stars to 0 (low threat)\n",
        "\n",
        "        # If keywords are present, ensure a minimum score to avoid missing threats\n",
        "        if has_threat_keywords(text):\n",
        "            score = max(score, 0.1)  # Minimum score if keywords are present\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying text: {text[:50]}... | Error: {e}\")\n",
        "        return 0\n",
        "\n",
        "data[\"threat_score\"] = data[\"text\"].apply(classify_text)\n",
        "# Flag threats based on score > 0.05 AND keywords\n",
        "data[\"is_threat\"] = (data[\"threat_score\"] > 0.05) & data[\"text\"].apply(has_threat_keywords)\n",
        "\n",
        "# Print all posts with threat scores\n",
        "print(\"All Posts with Threat Scores:\")\n",
        "print(data[[\"platform\", \"text\", \"threat_score\", \"is_threat\"]])\n",
        "\n",
        "# Step 4.2: Entity Extraction (Locations)\n",
        "print(\"\\nExtracting locations...\")\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "def truncate_text_for_ner(text, max_len=max_length):\n",
        "    encoded = ner_tokenizer.encode(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    decoded = ner_tokenizer.decode(encoded[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "def extract_entities(text):\n",
        "    try:\n",
        "        truncated_text = truncate_text_for_ner(text)\n",
        "        entities = ner(truncated_text)\n",
        "        locations = [e[\"word\"] for e in entities if e[\"entity_group\"] == \"LOC\"]\n",
        "        cleaned_locations = [re.sub(r'##', '', loc) for loc in locations]\n",
        "        return \", \".join(cleaned_locations) if cleaned_locations else \"N/A\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting entities from text: {text[:50]}... | Error: {e}\")\n",
        "        return \"N/A\"\n",
        "\n",
        "data[\"locations\"] = data[\"text\"].apply(extract_entities)\n",
        "print(\"Posts with Extracted Locations:\")\n",
        "print(data[[\"platform\", \"text\", \"locations\"]])\n",
        "\n",
        "# Save the analyzed data\n",
        "data.to_csv(\"reddit_osint_analyzed.csv\", index=False)\n",
        "print(\"Analyzed data saved to reddit_osint_analyzed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-oNdxq2UBPIh",
        "outputId": "4296de99-0aaa-43c8-ebf9-9ca9aa348628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'reddit_osint_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d554928b7d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load data (limited to 20 posts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reddit_osint_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_osint_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CyOWVwkBQVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}